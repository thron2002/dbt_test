cat > scripts/process_with_spark.py << 'EOF'
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, datediff, current_date

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Data Processing Demo") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.1") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .master("local[*]") \
    .getOrCreate()

print("Spark session created")

# Set S3A configuration
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "http://localhost:9000")
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "minioadmin")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "minioadmin")
spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")

# Read data from Minio
customers_df = spark.read.csv("s3a://raw-data/customers.csv", header=True, inferSchema=True)
orders_df = spark.read.csv("s3a://raw-data/orders.csv", header=True, inferSchema=True)
products_df = spark.read.csv("s3a://raw-data/products.csv", header=True, inferSchema=True)

print("Data read successfully from Minio")

# Transform customers data
customers_df = customers_df.withColumn("signup_date", to_date("signup_date", "yyyy-MM-dd"))
customers_df = customers_df.withColumn("last_purchase_date", to_date("last_purchase_date", "yyyy-MM-dd"))
customers_df = customers_df.withColumn("days_since_last_purchase", datediff(current_date(), col("last_purchase_date")))

# Transform orders data
orders_df = orders_df.withColumn("order_date", to_date("order_date", "yyyy-MM-dd"))

# Process product data
products_df = products_df.withColumn("price", col("price").cast("double"))

# Create a denormalized orders view with customer information
orders_enriched_df = orders_df.join(
    customers_df.select("id", "first_name", "last_name", "email"),
    orders_df["customer_id"] == customers_df["id"],
    "inner"
).drop(customers_df["id"])

# Store the processed data in PostgreSQL
jdbc_url = "jdbc:postgresql://localhost:5432/demo"
jdbc_properties = {
    "user": "postgres",
    "password": "postgres",
    "driver": "org.postgresql.Driver"
}

# Write the processed data to PostgreSQL
customers_df.write.jdbc(
    url=jdbc_url,
    table="raw_customers",
    mode="overwrite",
    properties=jdbc_properties
)

orders_df.write.jdbc(
    url=jdbc_url,
    table="raw_orders",
    mode="overwrite",
    properties=jdbc_properties
)

products_df.write.jdbc(
    url=jdbc_url,
    table="raw_products",
    mode="overwrite",
    properties=jdbc_properties
)

orders_enriched_df.write.jdbc(
    url=jdbc_url,
    table="raw_orders_enriched",
    mode="overwrite",
    properties=jdbc_properties
)

print("Data processed and loaded into PostgreSQL successfully!")
spark.stop()
EOF

# Execute the script
python scripts/process_with_spark.py
